{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoConfig,\n",
    ")\n",
    "import numpy as np\n",
    "from telewrap import Telewrap\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bert-base-uncased\"\n",
    "dataset_id = \"ag_news\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_id, num_labels=len(np.unique(dataset['test']['label'])))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-base\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True, \n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=8,\n",
    "    eval_steps=8,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='937' max='937' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [937/937 55:57, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.414600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.323300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.198300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>0.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>0.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>0.202600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>0.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.196200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.176600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>0.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>0.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>0.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>0.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>0.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>0.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>0.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>0.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>0.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.156800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>0.193200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>0.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>0.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>0.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>0.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Telewrap():\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9394736842105263"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(preds.label_ids, np.argmax(preds.predictions,axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
